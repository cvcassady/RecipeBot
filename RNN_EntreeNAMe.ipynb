{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random, json, string, pickle\n",
    "import keras\n",
    "import keras.layers\n",
    "import keras.models\n",
    "import keras.optimizers\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "import keras.layers.embeddings\n",
    "import keras.preprocessing.text\n",
    "import keras.preprocessing.sequence\n",
    "import keras.callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'ids_labels_nomissing_500_titles.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-45072418c0f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfood2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2food\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weights2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"ids_labels_nomissing_500_titles.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'ids_labels_nomissing_500_titles.p'"
     ]
    }
   ],
   "source": [
    "[ids, titles, labels, food2id, id2food, class_weights, class_weights2] = pickle.load( open( \"ids_labels_nomissing_500_titles.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabularySize = 1000 \n",
    "# Split Title into words, and define a vocabulary with the most common words.\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(nb_words = vocabularySize, \\\n",
    "                                               filters = '!\"#$%&()*+,-./:;<=>?@\\\\^_`{|}~\\t\\n')\n",
    "#Fit on titleStrings \n",
    "tokenizer.fit_on_texts(titleStrings)\n",
    "\n",
    "# Convert the title names into sequences of word ids using our vocabulary.\n",
    "titleSequences = tokenizer.texts_to_sequences(titleStrings)\n",
    "\n",
    "# Keep dictionaries that map ids -> words, and words -> ids.\n",
    "word2id = tokenizer.word_index\n",
    "id2word = {idx: word for (word, idx) in word2id.items()}\n",
    "# Find the sentence with most words.\n",
    "maxTitleLength = max([len(seq) for seq in titleSequences]) \n",
    "\n",
    "#pad sequences\n",
    "data = keras.preprocessing.sequence.pad_sequences(titleSequences, \n",
    "           maxlen = (maxTitleLength + 1), padding = 'post', truncating = 'post')\n",
    "\n",
    "id2word[0] = 'END'\n",
    "word2id['END'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(data.shape)  \n",
    "print(string.join([id2word[idx] for idx in data[0]], \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'maxTitleLength' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6db0fadd8a60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Define Shape of input batchSize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#output of Previous model X maxTitleLength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrecipes_title\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4096\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmaxTitleLength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#recipes_title = keras.layers.Input(batch_shape=(4096 + 500), name = \"input\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'maxTitleLength' is not defined"
     ]
    }
   ],
   "source": [
    "#build model...\n",
    "\n",
    "#Define Shape of input batchSize\n",
    "#output of Previous model X maxTitleLength\n",
    "recipes_title = keras.layers.Input(batch_shape=(4096 + 500 , maxTitleLength), name = \"input\")\n",
    "#recipes_title = keras.layers.Input(batch_shape=(4096 + 500), name = \"input\")\n",
    "\n",
    "\n",
    "# Build a matrix of size vocabularySize x 300 where each row corresponds to a \"word embedding\" vector.\n",
    "# This layer will convert replace each word-id with a word-vector of size 300.\n",
    "embeddings = keras.layers.embeddings.Embedding(vocabularySize, 300, name = \"embeddings\")(recipes_title)\n",
    "\n",
    "# Pass the word-vectors to the LSTM layer.\n",
    "# We are setting the hidden-state size to 512.\n",
    "hiddenStates = keras.layers.SimpleRNN(512, return_sequences = True, input_shape=(maxTitleLength, 300), name = \"rnn\")(embeddings)\n",
    "\n",
    "# Apply a linear (Dense) layer of size 512 x 256 to the outputs of the LSTM at each time step.\n",
    "denseOutput = TimeDistributed(keras.layers.Dense(vocabularySize), name = \"linear\")(hiddenStates)\n",
    "predictions = TimeDistributed(keras.layers.Activation(\"softmax\"), name = \"softmax\")(denseOutput)\n",
    "\n",
    "# Build the computational graph by specifying the input, and output of the network.\n",
    "model = keras.models.Model(input = recipes_title, output = predictions)\n",
    "\n",
    "#For Recurrent Neural Networks, a type of\n",
    "# optimization called RMSprop is preferred instead of the standard SGD udpates.\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer = keras.optimizers.RMSprop(lr = 0.001))\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train Model\n",
    "\n",
    "inputData = data[:, :-1]  # words 1, 2, 3, ... , (n-1)\n",
    "outputData = data[:, 1:]  # words 2, 3, 4, ... , (n)\n",
    "\n",
    "outputLabels = np.expand_dims(outputData, -1)\n",
    "\n",
    "\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(filepath=\"my_weights_RNN.hdf5\", save_weights_only = True, \\\n",
    "                                               save_best_only = True, monitor = 'loss')\n",
    "model.fit(inputData, outputLabels, batch_size = 256, nb_epoch = 10, callbacks = [checkpointer])\n",
    "\n",
    "model.save_weights('my_recipe_model.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build Inference model\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vislang]",
   "language": "python",
   "name": "conda-env-vislang-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
